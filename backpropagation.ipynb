{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "「誤差伝播法ってなぁに？」  \n",
    "「ニューラルネットワークの学習時に、パラメータの勾配を求める為の手法だよ」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合成関数の微分\n",
    "\n",
    "高校でやるやつ。これが分かれば誤差逆伝播法なんてほとんど理解したようなもん\n",
    "\n",
    "<br>\n",
    "\n",
    "試しに、以下の関数を微分してみよう。\n",
    "\n",
    "$$\n",
    "y = (x + 1)^2\n",
    "$$\n",
    "\n",
    "これは普通に展開しても解けるけど、合成関数の微分を使っても解けるね\n",
    "\n",
    "$$\n",
    "u = x + 1 \\\\\n",
    "y = u^2 \\\\\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx} = 2u \\cdot 1 = 2(x + 1)\n",
    "$$\n",
    "\n",
    "こんな感じで、**関数の関数**(合成関数)を微分するとき、関数ごとに微分をしたものをかけ合わせればよかった\n",
    "\n",
    "<br>\n",
    "\n",
    "じゃあここまでの流れをPythonで実装してみよう\n",
    "\n",
    "以下の二つの関数を`class`として実装する。\n",
    "- $f(x) = x + 1$\n",
    "- $g(x) = x^2$\n",
    "\n",
    "`class`にする必要ある？関数でよくね？と思うかもしれんが、まあ読んでみてよ。  \n",
    "まずは$f(x) = x + 1$から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus1:\n",
    "    def __call__(self, x):\n",
    "        return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出来た。入力した値に1を足して出力するだけの関数。`__call__()`というのは特殊メソッドで、関数のように`()`をつけて呼び出したときに実行されるヤツ。  \n",
    "こんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "plus1 = Plus1() # インスタンス生成\n",
    "y = plus1(3) # 関数の呼び出し\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力した3に1を足した4が出力された。\n",
    "\n",
    "では、今度は微分を行うメソッドを書いてみよう。「微分を行う」というのを、「入力された値に微分した値をかけて出力する」と捉えるとこうなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus1:\n",
    "    def __call__(self, x):\n",
    "        return x + 1\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`というメソッドを追加した。これは逆伝播という意味。  \n",
    "$x + 1$を$x$で微分すると1になるので、入力値(d)に1をかけて出力させる。\n",
    "\n",
    "こんなノリで、$g(x)$の方も書いちゃおう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x ** 2\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * self.x*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。$g(x) = x^2$を微分すると$2x$になるので、それを入力値(d)にかけて出力する。微分するときに使うので、初めに入力された値(x)は変数に保存しておく。\n",
    "\n",
    "ではこれらを使って、実際に計算してみよう。  \n",
    "ここから、\n",
    "\n",
    "$$\n",
    "h(x) = (x + 1)^2\n",
    "$$\n",
    "\n",
    "とおく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# インスタンス生成\n",
    "plus1 = Plus1()\n",
    "square = Square()\n",
    "\n",
    "# 計算\n",
    "x = 3\n",
    "u = plus1(x)\n",
    "y = square(u)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "でた。$h(3) = 16$ということで、正解！\n",
    "\n",
    "じゃあ今度は$h$の$x=3$での傾きを求めてみよう。$h'(3)$のことだね。  \n",
    "そしてこれはこんな感じで求められる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "d = square.backward(1)\n",
    "d = plus1.backward(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h'(x) = 2(x + 1) \\\\\n",
    "h'(3) = 8\n",
    "$$\n",
    "\n",
    "ということで正解！  \n",
    "これは合成関数の微分に基づいていて、正に「関数ごとの微分を掛け合わせる」という部分に当たる。\n",
    "\n",
    "> 入力された値に微分した値をかけて出力する\n",
    "\n",
    "さっきこう捉えた意味が分かったかな...?  \n",
    "微分した結果を後ろの方に伝えていく感じだね。一番初めは1を入力しておく。\n",
    "\n",
    "<br>\n",
    "\n",
    "こんな感じで、複数の関数を経て出力された値を何らかの変数で微分した値は、関数ごとに微分をすれば簡単に求まる。\n",
    "んで、これはニューラルネットワークが持つパラメータの勾配を求めるときにも使える。\n",
    "\n",
    "損失をパラメータで微分した値(勾配)を求めるとき、損失を出す際に通った**層**や**損失関数**を一つ一つ微分すればいいよねという話。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの構築\n",
    "\n",
    "ということで、誤差逆伝播法で学習を行うニューラルネットワークを作ってみようじゃないか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 層の定義\n",
    "\n",
    "NNは複数の層から構成されるので、まずは層を作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "\n",
    "パラメータを持つ層は工夫が必要なので、一旦パラメータを持たない層を作ってみよう。  \n",
    "さっきの関数と同じように作ればOK\n",
    "\n",
    "ちなみにReLUはこういう関数\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x \\leq 0) \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * (self.x > 0)\n",
    "\n",
    "    def update(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。先程実装したような一般的な関数の入出力は「数値」だが、NNの層の入出力は「ベクトル（というかテンソル）」で行うのでnumpyを使う。  \n",
    "あと、後々のことを考えて、パラメータを更新するメソッド`update()`を書いている。ReLUはパラメータを持たないので何もしないけど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全結合層\n",
    "\n",
    "全結合層。kerasでいうDense。PyTorchでいうLinear。ここではPyTorchに倣ってLinearにしよー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.w = np.random.randn(n_input, n_output)\n",
    "        self.b = np.random.randn(n_output)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.grad_w = np.dot(self.x.T, d)\n",
    "        self.grad_b = np.sum(d, axis=0)\n",
    "        return np.dot(d, self.w.T)\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.w -= lr * self.grad_W\n",
    "        self.b -= lr * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079aedbe56a1b5173cab2118ff2d67cefb61cc02a7283fc1dc9917aa8b2aaaf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
