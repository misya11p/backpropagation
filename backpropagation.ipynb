{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "「誤差伝播法ってなぁに？」  \n",
    "「ニューラルネットワークの学習時に、パラメータの勾配を求める為の手法だよ」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合成関数の微分\n",
    "\n",
    "高校でやるやつ。これが分かれば誤差逆伝播法なんてほとんど理解したようなもん\n",
    "\n",
    "<br>\n",
    "\n",
    "試しに、以下の関数を微分してみよう。\n",
    "\n",
    "$$\n",
    "y = (x + 1)^2\n",
    "$$\n",
    "\n",
    "これは普通に展開しても解けるけど、合成関数の微分を使っても解けるね\n",
    "\n",
    "$$\n",
    "u = x + 1 \\\\\n",
    "y = u^2 \\\\\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx} = 2u \\cdot 1 = 2(x + 1)\n",
    "$$\n",
    "\n",
    "こんな感じで、**関数の関数**(合成関数)を微分するとき、関数ごとに微分をしたものをかけ合わせればよかった\n",
    "\n",
    "<br>\n",
    "\n",
    "じゃあここまでの流れをPythonで実装してみよう\n",
    "\n",
    "以下の二つの関数を`class`として実装する。\n",
    "- $f(x) = x + 1$\n",
    "- $g(x) = x^2$\n",
    "\n",
    "`class`にする必要ある？関数でよくね？と思うかもしれんが、まあ読んでみてよ。  \n",
    "まずは$f(x) = x + 1$から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus1:\n",
    "    def __call__(self, x):\n",
    "        return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出来た。入力した値に1を足して出力するだけの関数。`__call__()`というのは特殊メソッドで、関数のように`()`をつけて呼び出したときに実行されるヤツ。  \n",
    "こんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "plus1 = Plus1() # インスタンス生成\n",
    "y = plus1(3) # 関数の呼び出し\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力した3に1を足した4が出力された。\n",
    "\n",
    "では、今度は微分を行うメソッドを書いてみよう。「微分を行う」というのを、「(`backward()`に)入力された値に微分した値をかけて出力する」と捉えるとこうなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus1:\n",
    "    def __call__(self, x):\n",
    "        return x + 1\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`というメソッドを追加した。  \n",
    "$x + 1$を$x$で微分すると1になるので、入力値(d)に1をかけて出力させる。\n",
    "\n",
    "こんなノリで、$g(x)$の方も書いちゃおう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x ** 2\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * self.x*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。$g(x) = x^2$を微分すると$2x$になるので、それを入力値(d)にかけて出力する。微分するときに使うので、初めに入力された値(x)は変数に保存しておく。\n",
    "\n",
    "<br>\n",
    "\n",
    "ちなみに、`__call__()`で行なっている演算は**順伝播**  \n",
    "`backward()`で行なっている演算は**逆伝播**という\n",
    "\n",
    "あと、順伝播(`__call__()`)への入力と逆伝播(`backward()`)への入力が混ざる気がするので、以下のように区別する。  \n",
    "- 順伝播: **入力(x)**\n",
    "- 逆伝播: **入力(d)**\n",
    "\n",
    "<br>\n",
    "\n",
    "ではこれらを使って、実際に計算してみよう。  \n",
    "ここから、\n",
    "\n",
    "$$\n",
    "h(x) = (x + 1)^2\n",
    "$$\n",
    "\n",
    "とおく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# インスタンス生成\n",
    "plus1 = Plus1()\n",
    "square = Square()\n",
    "\n",
    "# 計算\n",
    "x = 3\n",
    "u = plus1(x)\n",
    "y = square(u)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "でた。$h(3) = 16$ということで、正解！\n",
    "\n",
    "じゃあ今度は$h$の$x=3$での傾きを求めてみよう。$h'(3)$のことだね。  \n",
    "そしてこれはこんな感じで求められる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "d = square.backward(1)\n",
    "d = plus1.backward(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h'(x) = 2(x + 1) \\\\\n",
    "h'(3) = 8\n",
    "$$\n",
    "\n",
    "ということで正解！  \n",
    "これは合成関数の微分に基づいていて、正に「関数ごとの微分を掛け合わせる」という部分に当たる。\n",
    "\n",
    "> 入力された値に微分した値をかけて出力する\n",
    "\n",
    "さっきこう捉えた意味が分かったかな...?  \n",
    "微分した結果を後ろの方に伝えていく感じだね。一番初めは1を入力しておく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの構築\n",
    "\n",
    "という感じで、複数の関数を経て出力された値を何らかの変数で微分した値は、関数ごとに微分をすれば簡単に求まる。\n",
    "んで、これはニューラルネットワークが持つパラメータの勾配を求めるときにも使える。\n",
    "\n",
    "損失をパラメータで微分した値(勾配)を求めるとき、損失を出す際に通った**層**や**損失関数**を一つ一つ微分すればいいよねという話。\n",
    "\n",
    "<br>\n",
    "\n",
    "じゃあ、誤差逆伝播法で学習を行うニューラルネットワークを実際に作ってみようじゃないか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 層の定義\n",
    "\n",
    "NNは複数の層から構成されるので、まずは層を作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "\n",
    "パラメータを持つ層は工夫が必要なので、一旦パラメータを持たない層を作ってみよう。  \n",
    "さっきの関数と同じように作ればOK\n",
    "\n",
    "ちなみにReLUはこういう関数\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x \\leq 0) \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * (self.x > 0)\n",
    "\n",
    "    def update(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。bool型は`+`とか`*`みたいな演算子と一緒に使うと0,1として扱ってくれるので、`backward()`はこういう書き方でOK。入力が0を超えてたら傾き1、それ以外は0。\n",
    "\n",
    "さっき実装したような一般的な関数の入出力は「数値」だけど、NNの層の入出力は「ベクトル（というかテンソル）」で行うのでnumpyを使う。  \n",
    "あと、後々のことを考えて、パラメータを更新するメソッド`update()`を書いている。ReLUはパラメータを持たないので何もしないけど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全結合層\n",
    "\n",
    "全結合層。kerasでいうDense。PyTorchでいうLinear。ここではPyTorchに倣ってLinearにしよー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.w = np.random.randn(n_input, n_output)\n",
    "        self.b = np.random.randn(n_output)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.grad_w = np.multiply(*np.meshgrid(self.x, d))\n",
    "        self.grad_b = d\n",
    "        return np.dot(d, self.w.T)\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.w -= lr * self.grad_W\n",
    "        self.b -= lr * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。ミニバッチは非対応にした。そのせいで`grad_w`が複雑になっているけど、「逆伝播」はミニバッチじゃない方が分かりやすい気がしたので。\n",
    "\n",
    "パラメータを持つ層なので、`__init__()`でパラメータを初期化。んで`backward()`(逆伝播)の時に、入力(d)を各パラメータで微分して変数に入れておく。  \n",
    "それらはパラメータの勾配となるので、`update()`(パラメータ更新)の時は、それらに学習率をかけてパラメータから引く感じ。\n",
    "\n",
    "<br>\n",
    "\n",
    "< `backward()`(逆伝播)について >\n",
    "\n",
    "まず、順伝播時の入力(x)を$x$、出力を$y$、重みを$w$、バイアスを$b$として、これらの関係を表す\n",
    "\n",
    "$$\n",
    "w \\cdot x + b = y\n",
    "$$\n",
    "\n",
    "内積にバイアスを足しているね\n",
    "\n",
    "これを踏まえて、ここ(逆伝播)で求める値を説明する。求める値は3つ。\n",
    "\n",
    "- 重みの勾配(grad_w)\n",
    "\n",
    "l番目の入力($x_l$)からm番目の出力($y_m$)への重みを$w_{lm}$とすると、求めたいのは$y$を$w_{lm}$で微分したもの。  \n",
    "で、それは$x_l \\times y_m$。l、mではない部分やバイアスは微分をする上で関係ないので無視してよくて、残るのは$x_l \\times w_{lm}$の部分だけ。\n",
    "\n",
    "これを全てのlmで求めるのがこのコード: `np.multiply(*np.meshgrid(self.x, d))`  \n",
    "これは、全部の組み合わせで掛け算をしているだけ。↓の例を見て理解してくれ。\n",
    "\n",
    "```python\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5]\n",
    "y = np.multiply(*np.meshgrid(a, b))\n",
    "print(y)\n",
    "\n",
    ">> [[ 4  8 12]\n",
    "    [ 5 10 15]]\n",
    "```\n",
    "\n",
    "\n",
    "- バイアスの勾配(grad_b)\n",
    "\n",
    "バイアスで微分する上で内積の部分は関係ないので無視してよい。上の式を$b$で微分すると1なので、バイアス(b)の勾配は、入力(d)に1をかけたd\n",
    "\n",
    "- 入力(x)の勾配(return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "\n",
    "ではこれらを組み合わせてNNをつくろー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, d):\n",
    "        for layer in self.layers[::-1]:\n",
    "            d = layer.backward(d)\n",
    "\n",
    "    def update(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これだけ。かんたん。\n",
    "\n",
    "使い方は、インスタンス生成時にレイヤーを入れていくだけ。kerasとかPyTorchでいうSequentialみたいな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    Linear(5, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 10,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでOK。入力する値は5個、出てくる値は10個。適当に乱数を入れてみると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.09094691 16.34657847 -5.13068172  1.63253641 -4.44916527  0.57316053\n",
      " -8.9245998   6.81595766  7.14740493  7.01184702]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(5)\n",
    "y = nn(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適当な値が10個出てきた。おーけー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "\n",
    "損失関数もクラスとして書いてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二乗和誤差\n",
    "\n",
    "差の二乗の和...を、2で割ったもの。\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i} (y_i - t_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "なんで2で割るかって？　微分した時に綺麗になるからだよ〜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS:\n",
    "    def __call__(self, y, t):\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        return np.sum((y - t) ** 2) / 2\n",
    "\n",
    "    def backward(self):\n",
    "        return self.y - self.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`の中身、綺麗でしょ〜。微分すると二乗の部分が前に出てくるので、$\\frac{1}{2}$と打ち消し合っていい感じになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交差エントロピー\n",
    "\n",
    "分類タスクで使うやつ。これを使うときは、出力層の活性化関数はsoftmaxを使う。ので、それも一緒に書いちゃおう！  \n",
    "softmaxにかける前のベクトルを入力するという前提で書く！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __call__(self, y, t):\n",
    "        prob = self._softmax(y)\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        loss = -np.sum(t * np.log(y))\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return self.y - self.t\n",
    "\n",
    "    def _softmax(self, y):\n",
    "        return np.exp(y) / np.sum(np.exp(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "実際に学習させてみよう。定番のMNIST。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット\n",
    "\n",
    "kerasからもってくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m mnist\n\u001b[1;32m      2\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39mload_data()\n",
      "File \u001b[0;32m~/Private/study/backpropagation/.venv/lib/python3.9/site-packages/keras/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n",
      "File \u001b[0;32m~/Private/study/backpropagation/.venv/lib/python3.9/site-packages/keras/distribute/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[0;32m~/Private/study/backpropagation/.venv/lib/python3.9/site-packages/keras/distribute/sidecar_evaluator.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Python module for evaluation loop.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_logging \u001b[39mas\u001b[39;00m logging\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d846107b04e8675d5251d5490c50179a863ca33ff1b32ba3415a48b925325c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
