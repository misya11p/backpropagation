{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "「誤差伝播法ってなぁに？」  \n",
    "「ニューラルネットワークの学習時に、パラメータの勾配を求める為の手法だよ」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合成関数の微分\n",
    "\n",
    "高校でやるやつ。これが分かれば誤差逆伝播法なんてほとんど理解したようなもん\n",
    "\n",
    "<br>\n",
    "\n",
    "試しに、以下の関数を微分してみよう。\n",
    "\n",
    "$$\n",
    "y = (x + 1)^2\n",
    "$$\n",
    "\n",
    "これは普通に展開しても解けるけど、合成関数の微分を使っても解けるね\n",
    "\n",
    "$$\n",
    "u = x + 1 \\\\\n",
    "y = u^2 \\\\\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx} = 2u \\cdot 1 = 2(x + 1)\n",
    "$$\n",
    "\n",
    "こんな感じで、**関数の関数**(合成関数)を微分するときは、関数ごとに微分をしたものをかけ合わせればよかった\n",
    "\n",
    "<br>\n",
    "\n",
    "じゃあここまでの流れをPythonで実装してみよう\n",
    "\n",
    "以下の二つの関数を`class`として実装する。\n",
    "- $f(x) = x + 1$\n",
    "- $g(x) = x^2$\n",
    "\n",
    "`class`にする必要ある？関数でよくね？と思うかもしれんけど、まあ読んでみてよ。  \n",
    "まずは$f(x) = x + 1$から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus1:\n",
    "    def __call__(self, x):\n",
    "        return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出来た。入力した値に1を足して出力するだけの関数。`__call__()`というのは特殊メソッドで、関数のように`()`をつけて呼び出したときに実行されるヤツ。  \n",
    "こんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "plus1 = Plus1() # インスタンス生成\n",
    "y = plus1(3) # # __call__メソッドの呼び出し\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力した3に1を足した4が出力された。\n",
    "\n",
    "では、今度は微分を行うメソッドを書いてみよう。「微分を行う」というのを、「(`backward()`に)入力された値に微分した値をかけて出力する」と捉えるとこうなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus1:\n",
    "    def __call__(self, x):\n",
    "        return x + 1\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`というメソッドを追加した。  \n",
    "$x + 1$を$x$で微分すると1になるので、入力値(d)に1をかけて出力させる。\n",
    "\n",
    "こんなノリで、$g(x) = x^2$の方も書いちゃおう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x ** 2\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * self.x*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。$g(x)$を$x$で微分すると$2x$になるので、それを入力値(d)にかけて出力する。微分するときに使うので、初めに入力された値(x)は変数に保存しておく。  \n",
    "こんな感じに、`backward()`を追加したり、値を一時的に保存したりしたかったから、classとして実装したのだ\n",
    "\n",
    "<br>\n",
    "\n",
    "ちなみに、`__call__()`で行なっている演算は**順伝播**  \n",
    "`backward()`で行なっている演算は**逆伝播**という\n",
    "\n",
    "あと、順伝播(`__call__()`)への入力と逆伝播(`backward()`)への入力が混ざる気がするので、今後以下のように区別する。  \n",
    "- 順伝播への入力: **入力(x)**\n",
    "- 逆伝播への入力: **入力(d)**\n",
    "\n",
    "<br>\n",
    "\n",
    "ではこれらを使って、実際に計算してみよう。  \n",
    "\n",
    "ここから、\n",
    "\n",
    "$$\n",
    "h(x) = (x + 1)^2\n",
    "$$\n",
    "\n",
    "としよう\n",
    "\n",
    "まずは順伝播から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# インスタンス生成\n",
    "plus1 = Plus1()\n",
    "square = Square()\n",
    "\n",
    "# 計算\n",
    "x = 3\n",
    "u = plus1(x)\n",
    "y = square(u)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ほい。$h(3) = 16$ということで、正解！\n",
    "\n",
    "じゃあ今度は逆伝播で、$h$の$x=3$での傾きを求めてみよう。$h'(3)$のことだね。  \n",
    "そしてこれはこんな感じで求められる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "d = square.backward(1)\n",
    "d = plus1.backward(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h'(x) = 2(x + 1) \\\\\n",
    "h'(3) = 8\n",
    "$$\n",
    "\n",
    "ということで正解！  \n",
    "これは合成関数の微分に基づいていて、正に「関数ごとの微分を掛け合わせる」という部分に当たるね！\n",
    "\n",
    "> 入力された値に微分した値をかけて出力する\n",
    "\n",
    "さっきこう捉えた意味が分かったかな...?  \n",
    "微分した結果を逆方向に伝えていく感じだね。一番初めは1を入力しておく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの構築\n",
    "\n",
    "見てもらった通り、複数の関数を経て出力された値を何らかの変数で微分した値は、関数ごとに微分をすれば簡単に求まる。  \n",
    "んで、これはニューラルネットワークが持つパラメータの勾配を求めるときにも使える。\n",
    "\n",
    "損失をパラメータで微分した値(勾配)を求めるとき、損失を出す際に通った**層**や**損失関数**を一つ一つ微分すればいいよねという話。\n",
    "\n",
    "<br>\n",
    "\n",
    "じゃあ、誤差逆伝播法で学習を行うニューラルネットワークを実際に作ってみようじゃないか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 層の定義\n",
    "\n",
    "NNは複数の層から構成されるので、まずは層を作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "\n",
    "重みとかバイアスみたいなパラメータを持つ層は工夫が必要なので、一旦パラメータを持たない層を作ってみよう。  \n",
    "さっきの関数と同じように作ればOK\n",
    "\n",
    "ちなみにReLUはこういう関数\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x \\leq 0) \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, d):\n",
    "        return d * (self.x > 0)\n",
    "\n",
    "    def update(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。bool型は`+`とか`*`みたいな演算子と一緒に使うと0,1として扱ってくれるので、`backward()`はこういう書き方でOK。入力が0を超えてたら傾き1、それ以外は0。\n",
    "\n",
    "さっき実装したような一般的な関数の入出力は「数値」だけど、NNの層の入出力は「ベクトル（というかテンソル）」で行うのでnumpyを使うよ  \n",
    "あと、後々のことを考えて、パラメータを更新するメソッド`update()`を書いている。ReLUはパラメータを持たないので何もしないけど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全結合層\n",
    "\n",
    "全結合層。kerasでいうDense。PyTorchでいうLinear。ここではPyTorchに倣ってLinearにしよー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.w = np.random.normal(size=(n_input, n_output), scale=np.sqrt(2/n_output))\n",
    "        self.b = np.random.randn(n_output)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def backward(self, d):\n",
    "        self.grad_w = np.multiply(*np.meshgrid(d, self.x))\n",
    "        self.grad_b = d\n",
    "        return np.dot(d, self.w.T)\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.w -= lr * self.grad_w\n",
    "        self.b -= lr * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。ミニバッチは非対応にした。そのせいで`grad_w`が複雑になっているけど、「逆伝播」についてはミニバッチじゃない方が分かりやすい気がしたので。\n",
    "\n",
    "パラメータを持つ層なので、`__init__()`でパラメータを初期化している。  \n",
    "重みもバイアスも正規分布に従って初期化してるけど、重みの初期化は`randn()`ではなく`normal()`を使っている。  \n",
    "`randn()`だと標準偏差が1になるんだけど、それだと重みが大きくなりすぎて上手くいかなかったので、小さくしてる。詳しくは**Xaivierの初期化**とかでググってみて。\n",
    "\n",
    "`backward()`は、入力(d)を各パラメータで微分して変数に入れておく。  \n",
    "それらはパラメータの勾配となるので、`update()`(パラメータ更新)の時は、それらに学習率をかけてパラメータから引く感じ。\n",
    "\n",
    "<br>\n",
    "\n",
    "< `backward()`(逆伝播)について >\n",
    "\n",
    "まず、あるニューロンに対する順伝播時の入力(x)を$x$、出力を$y$、重みを$w$、バイアスを$b$として、これらの関係を表す。\n",
    "\n",
    "$$\n",
    "w \\cdot x + b = y\n",
    "$$\n",
    "\n",
    "内積にバイアスを足しているね\n",
    "\n",
    "これを踏まえて、ここ(逆伝播)で求める値を説明する。求める値は3つ。\n",
    "\n",
    "- 重みの勾配(grad_w)\n",
    "\n",
    "$l$番目の入力($x_l$)から$m$番目の出力($y_m$)への重みを$w_{lm}$とすると、求めたいのは$y$を$w_{lm}$で微分したもの。  \n",
    "で、それは入力(d)に$x_l$をかけたものになる。$l, m$ではない部分やバイアスは微分をする上で関係ないので無視してよくて、残るのは$x_l \\times w_{lm}$の部分だけという感じ。これを$w_{lm}$で微分すると$x_l$だけ残る。\n",
    "\n",
    "これを全ての$l, m$で求めるのがこのコード: `np.multiply(*np.meshgrid(self.x, d))`  \n",
    "これは、全部の組み合わせで掛け算をしているだけ。↓の例を見て理解してくれ。\n",
    "\n",
    "```python\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5]\n",
    "y = np.multiply(*np.meshgrid(a, b))\n",
    "print(y)\n",
    "\n",
    ">> [[ 4  8 12]\n",
    "    [ 5 10 15]]\n",
    "```\n",
    "\n",
    "(これ一発でできる関数とか知っている人いたら教えてほしい...)\n",
    "\n",
    "\n",
    "- バイアスの勾配(grad_b)\n",
    "\n",
    "バイアスで微分する上で内積の部分は関係ないので無視してよい。上の式を$b$で微分すると1なので、バイアス(b)の勾配は、入力(d)に1をかけたd\n",
    "\n",
    "- 入力(x)の勾配(return)\n",
    "\n",
    "ある一つの入力(x)に着目する。この入力(x)は$x_i$としよう。んで、$x_i$は層の全てのニューロン(の演算)に関わっている。  \n",
    "ここで、層のニューロンの数が一つだと仮定しよう。そうすると話は簡単。重みの勾配を求めたときと逆のことをすればいいので、$x_i$の勾配はそこに対応する重み$w_i$となる。  \n",
    "でも実際は層のニューロンが複数あるので、勾配が複数出てきそう。そんなときにどうすればよいか。\n",
    "\n",
    "これは、実はとってもシンプルで、出てきた勾配を全部足せばよい。で、それは行列の積で上手くまとめられて、それがこれ: `np.dot(d, self.w.T)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "\n",
    "ではこれらを組み合わせてNNをつくろー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, d):\n",
    "        for layer in self.layers[::-1]:\n",
    "            d = layer.backward(d)\n",
    "\n",
    "    def update(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これだけ。かんたん。  \n",
    "使い方は、インスタンス生成時にレイヤーを入れていくだけ。kerasとかPyTorchでいうSequentialみたいな感じ。以下メソッドの説明\n",
    "\n",
    "- `__call__()`  \n",
    "順伝播。与えた層で順番に演算しているだけ。\n",
    "\n",
    "- `backward()`  \n",
    "逆伝播。リストに`[::-1]`をつけると逆順になる\n",
    "\n",
    "- `update()`  \n",
    "パラメータ更新。逆伝播で求めた勾配に学習率をかけてパラメータから引く。ここをシンプルにするために、パラメータを持たない層にもこのメソッドを書いておいた。\n",
    "\n",
    "<br>\n",
    "\n",
    "試しに使ってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    Linear(5, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 10,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの構築はこれでOK。入力する値は5個、出てくる値は10個。適当に乱数を5個入れてみると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.70924181 -1.15491763  2.64228401  1.00399676 -1.36650978 -2.33312378\n",
      "  2.08624605 -0.2996883   0.48605541  2.26515702]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(5)\n",
    "y = nn(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適当な値が10個出てきた。おーけー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "\n",
    "損失関数もクラスとして書いてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二乗和誤差\n",
    "\n",
    "差の二乗の和...を、2で割ったもの。\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i} (y_i - t_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "なんで2で割るかって？　微分した時に綺麗になるからだよ〜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS:\n",
    "    def __call__(self, y, t):\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        return np.sum((y - t) ** 2) / 2\n",
    "\n",
    "    def backward(self):\n",
    "        return self.y - self.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`の中身、綺麗でしょ〜。微分すると二乗の部分が前に出てくるので、$\\frac{1}{2}$と打ち消し合っていい感じになる  \n",
    "あと、ここの逆伝播では何も入力しない。逆伝播の一番最初の部分だから入力はないね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交差エントロピー\n",
    "\n",
    "分類タスクで使うやつ。これを使うときは、出力層の活性化関数はsoftmaxを使う。ので、それも一緒に書いちゃおう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __call__(self, y, t):\n",
    "        y = self._softmax(y)\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        loss = -np.sum(t * np.log(y))\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return self.y - self.t\n",
    "\n",
    "    def _softmax(self, y):\n",
    "        return np.exp(y) / np.sum(np.exp(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これも、逆伝播がとってもきれいな形。普通活性化関数はNN側に書くけど、交差エントロピーと一緒に使うとこの部分の微分が綺麗になるので、こっちに書いた。  \n",
    "微分がきれいなのはたまたまではなく、頭のいい人が綺麗になるように考えたからだと思う。\n",
    "\n",
    "<br>\n",
    "\n",
    "二つの損失関数を書いたけど、どちらも逆伝播が「予測値-正解」になっているね。  \n",
    "勾配を求めるときは、これをNNに入れて逆伝播を行うんだけど、「予測値-正解」って、正解と予測値の差だから、「誤差」と表せるよね。\n",
    "\n",
    "「誤差逆伝播法」の由来が分かった気がするね"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "では実際に学習させてみよう。定番のMNIST。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 00:19:22.331455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-20 00:19:22.455261: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-20 00:19:22.954952: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:\n",
      "2022-10-20 00:19:22.955084: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:\n",
      "2022-10-20 00:19:22.955090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 正規化\n",
    "x_train = x_train.reshape(-1, 784) / 255\n",
    "x_test = x_test.reshape(-1, 784) / 255\n",
    "\n",
    "# one-hotに\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率的勾配降下法で学習させる。ミニバッチに対応していないので、バッチサイズは1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, x, y, criterion, lr, n_epochs):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss = 0\n",
    "        for _ in tqdm(range(len(x)), desc=f'{epoch}epoch'):\n",
    "            # ランダムに一つ抽出\n",
    "            idx = np.random.randint(0, len(x))\n",
    "            sample_x = x[idx]\n",
    "            sample_y = y[idx]\n",
    "\n",
    "            out = model(sample_x) # 順伝播\n",
    "            loss += criterion(out, sample_y) # 損失の計算\n",
    "            d = criterion.backward() # 損失関数の逆伝播\n",
    "            model.backward(d) # ニューラルネットワークの逆伝播\n",
    "            model.update(lr) # パラメータの更新\n",
    "        print(f'loss: {loss / len(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワーク構成は適当！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    Linear(784, 512),\n",
    "    ReLU(),\n",
    "    Linear(512, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レッツゴー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1epoch: 100%|██████████| 60000/60000 [01:55<00:00, 517.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2695821597613696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2epoch: 100%|██████████| 60000/60000 [01:56<00:00, 516.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10846042071623153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3epoch: 100%|██████████| 60000/60000 [01:55<00:00, 521.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.07909254203673934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4epoch: 100%|██████████| 60000/60000 [01:54<00:00, 523.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05543957445079766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5epoch: 100%|██████████| 60000/60000 [01:53<00:00, 529.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.04660230423813134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(nn, x_train, y_train, CrossEntropy(), 0.001, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私の環境では、1エポックに2分ぐらいだった。cpuなので、こんなもん。\n",
    "\n",
    "いい感じにlossが減っているので上手くいってそう。確認してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_x = x_test[0]\n",
    "sample_y = y_test[0]\n",
    "t = np.argmax(sample_y)\n",
    "pred = np.argmax(nn(sample_x))\n",
    "plt.title(f'true: {t}, pred: {pred}')\n",
    "plt.imshow(sample_x.reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちゃんと7と予測できているね。\n",
    "\n",
    "精度も確かめてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "predicts = np.array([nn(x).argmax() for x in x_test])\n",
    "y_test = y_test.argmax(axis=1)\n",
    "print('accuracy:', (predicts == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチリ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079aedbe56a1b5173cab2118ff2d67cefb61cc02a7283fc1dc9917aa8b2aaaf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
